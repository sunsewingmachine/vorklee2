---
version: "5.4"
maintainer: "Vorklee2 Platform Engineering & SRE Team"
last_updated: "2025-01-15 18:30:00 UTC"
tier: "enterprise"
format: "markdown"
framework: "Next.js 14+"
database: "NeonDB"
compliance: ["SOC2", "GDPR", "HIPAA", "ISO 27001"]
---

# üìä 14_Advanced_Observability_and_Monitoring_v5.md
## Production-Grade Observability, Monitoring, Alerting, and SRE Best Practices

---

## üß≠ Purpose

This document defines **advanced observability and monitoring** for the **Vorklee2 Enterprise Platform (v5.4)** to achieve **100% production visibility**. It covers the three pillars of observability (Logs, Metrics, Traces), SLI/SLO/SLA definitions, incident management, and Site Reliability Engineering (SRE) practices.

---

## üèóÔ∏è 1. The Three Pillars of Observability

### Architecture Overview

```typescript
// OpenTelemetry Integration (Unified Observability)
import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';
import { Resource } from '@opentelemetry/resources';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';
import { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';
import { JaegerExporter } from '@opentelemetry/exporter-jaeger';
import { PrometheusExporter } from '@opentelemetry/exporter-prometheus';
import { WinstonInstrumentation } from '@opentelemetry/instrumentation-winston';

export class ObservabilityStack {
  /**
   * Initialize OpenTelemetry (Auto-instrumentation)
   */
  static initialize() {
    // Create provider with resource attributes
    const provider = new NodeTracerProvider({
      resource: new Resource({
        [SemanticResourceAttributes.SERVICE_NAME]: 'vorklee-notes',
        [SemanticResourceAttributes.SERVICE_VERSION]: '5.4.0',
        [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV,
        [SemanticResourceAttributes.SERVICE_INSTANCE_ID]: process.env.HOSTNAME,
      }),
    });

    // Configure exporters
    const jaegerExporter = new JaegerExporter({
      endpoint: 'http://jaeger-collector:14268/api/traces',
    });

    provider.addSpanProcessor(
      new BatchSpanProcessor(jaegerExporter, {
        maxQueueSize: 1000,
        scheduledDelayMillis: 5000,
      })
    );

    // Register provider
    provider.register();

    // Auto-instrument libraries
    this.autoInstrument();
  }

  /**
   * Auto-instrument popular libraries
   */
  private static autoInstrument() {
    registerInstrumentations({
      instrumentations: [
        new HttpInstrumentation(),
        new ExpressInstrumentation(),
        new PgInstrumentation(),
        new RedisInstrumentation(),
        new WinstonInstrumentation(),
        new FetchInstrumentation(),
      ],
    });
  }
}
```

---

## üìù 2. Structured Logging (Pillar 1)

### Log Levels and Standards

```typescript
// Structured Logging with Correlation IDs
import winston from 'winston';
import { v4 as uuidv4 } from 'uuid';

export class StructuredLogger {
  private logger: winston.Logger;

  constructor(service: string) {
    this.logger = winston.createLogger({
      level: process.env.LOG_LEVEL || 'info',
      format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.errors({ stack: true }),
        winston.format.json()
      ),
      defaultMeta: {
        service,
        environment: process.env.NODE_ENV,
        version: process.env.APP_VERSION,
      },
      transports: [
        // Console (for local dev)
        new winston.transports.Console({
          format: winston.format.combine(
            winston.format.colorize(),
            winston.format.simple()
          ),
        }),

        // File (JSON for parsing)
        new winston.transports.File({
          filename: 'logs/error.log',
          level: 'error',
          maxsize: 10485760, // 10MB
          maxFiles: 10,
        }),

        new winston.transports.File({
          filename: 'logs/combined.log',
          maxsize: 10485760,
          maxFiles: 30,
        }),

        // Elasticsearch (for SIEM)
        new ElasticsearchTransport({
          level: 'info',
          clientOpts: {
            node: process.env.ELASTICSEARCH_URL,
          },
          index: 'logs-vorklee',
        }),

        // Datadog (for APM)
        new DatadogTransport({
          apiKey: process.env.DATADOG_API_KEY,
          service,
        }),
      ],
    });
  }

  /**
   * Log with correlation ID (for distributed tracing)
   */
  log(level: string, message: string, meta: LogMeta = {}) {
    const correlationId = meta.correlationId || this.getCorrelationId();

    this.logger.log(level, message, {
      ...meta,
      correlationId,
      traceId: this.getTraceId(),
      spanId: this.getSpanId(),
      userId: this.getUserId(),
      orgId: this.getOrgId(),
      ip: this.getClientIP(),
      userAgent: this.getUserAgent(),
      timestamp: new Date().toISOString(),
    });
  }

  /**
   * Structured Error Logging
   */
  error(error: Error, context: ErrorContext = {}) {
    this.log('error', error.message, {
      error: {
        name: error.name,
        message: error.message,
        stack: error.stack,
        code: (error as any).code,
      },
      context,
      severity: this.calculateSeverity(error),
    });

    // Send to error tracking (Sentry)
    Sentry.captureException(error, {
      contexts: { custom: context },
    });
  }

  /**
   * Audit Logging (Compliance)
   */
  audit(event: AuditEvent) {
    this.log('info', `Audit: ${event.action}`, {
      audit: true,
      userId: event.userId,
      orgId: event.orgId,
      action: event.action,
      resource: event.resource,
      resourceId: event.resourceId,
      changes: event.changes,
      ipAddress: event.ipAddress,
      userAgent: event.userAgent,
      timestamp: event.timestamp,
      result: event.result, // success | failure
      reason: event.reason, // if failed
    });

    // Also send to dedicated audit log system
    await this.sendToAuditSystem(event);
  }

  /**
   * Performance Logging
   */
  performance(operation: string, duration: number, metadata: any = {}) {
    this.log('info', `Performance: ${operation}`, {
      performance: true,
      operation,
      duration,
      metadata,
      threshold: this.getPerformanceThreshold(operation),
      slow: duration > this.getPerformanceThreshold(operation),
    });
  }
}
```

### Log Aggregation Pipeline

```yaml
# Fluentd Configuration for Log Aggregation
# /etc/fluentd/fluent.conf

<source>
  @type tail
  path /var/log/vorklee/app.log
  pos_file /var/log/fluentd/app.log.pos
  tag vorklee.app
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%L%z
  </parse>
</source>

<filter vorklee.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    environment "#{ENV['NODE_ENV']}"
  </record>
</filter>

# Remove sensitive data (PII redaction)
<filter vorklee.**>
  @type redact
  <redact>
    key password
    method replace
    replacement "[REDACTED]"
  </redact>
  <redact>
    key ssn
    method replace
    replacement "[REDACTED]"
  </redact>
  <redact>
    key email
    method pattern
    pattern /^(.{2}).*(@.*)$/
    replacement '\1***\2'
  </redact>
</filter>

# Send to multiple destinations
<match vorklee.**>
  @type copy

  # Elasticsearch (for search/SIEM)
  <store>
    @type elasticsearch
    host elasticsearch.vorklee.internal
    port 9200
    logstash_format true
    logstash_prefix vorklee-logs
    include_tag_key true
    type_name _doc
    <buffer>
      @type file
      path /var/log/fluentd/buffer/elasticsearch
      flush_interval 5s
      retry_max_times 5
    </buffer>
  </store>

  # S3 (for long-term archival)
  <store>
    @type s3
    s3_bucket vorklee-logs-archive
    s3_region us-east-1
    path logs/%Y/%m/%d/
    time_slice_format %Y%m%d%H
    <buffer time>
      @type file
      path /var/log/fluentd/buffer/s3
      timekey 3600  # 1 hour
      timekey_wait 10m
    </buffer>
  </store>

  # Datadog (for APM)
  <store>
    @type datadog
    api_key "#{ENV['DATADOG_API_KEY']}"
    <buffer>
      @type memory
      flush_interval 10s
    </buffer>
  </store>
</match>
```

---

## üìà 3. Metrics and Monitoring (Pillar 2)

### Prometheus Metrics

```typescript
// Prometheus Metrics Instrumentation
import { register, Counter, Histogram, Gauge } from 'prom-client';

export class MetricsCollector {
  // HTTP Metrics
  static httpRequestsTotal = new Counter({
    name: 'http_requests_total',
    help: 'Total number of HTTP requests',
    labelNames: ['method', 'route', 'status_code', 'service'],
  });

  static httpRequestDuration = new Histogram({
    name: 'http_request_duration_seconds',
    help: 'Duration of HTTP requests in seconds',
    labelNames: ['method', 'route', 'status_code'],
    buckets: [0.01, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10], // P50, P95, P99
  });

  // Database Metrics
  static dbQueryDuration = new Histogram({
    name: 'db_query_duration_seconds',
    help: 'Duration of database queries in seconds',
    labelNames: ['query_type', 'table'],
    buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5],
  });

  static dbConnectionPoolSize = new Gauge({
    name: 'db_connection_pool_size',
    help: 'Current database connection pool size',
    labelNames: ['state'], // idle, active, waiting
  });

  static dbQueryErrors = new Counter({
    name: 'db_query_errors_total',
    help: 'Total number of database query errors',
    labelNames: ['error_type'],
  });

  // Cache Metrics
  static cacheHits = new Counter({
    name: 'cache_hits_total',
    help: 'Total number of cache hits',
    labelNames: ['cache_key_prefix'],
  });

  static cacheMisses = new Counter({
    name: 'cache_misses_total',
    help: 'Total number of cache misses',
    labelNames: ['cache_key_prefix'],
  });

  static cacheSize = new Gauge({
    name: 'cache_size_bytes',
    help: 'Current cache size in bytes',
  });

  // Business Metrics
  static notesCreated = new Counter({
    name: 'notes_created_total',
    help: 'Total number of notes created',
    labelNames: ['org_id'],
  });

  static activeUsers = new Gauge({
    name: 'active_users',
    help: 'Number of active users',
    labelNames: ['time_window'], // 1min, 5min, 1hour, 24hour
  });

  static apiErrors = new Counter({
    name: 'api_errors_total',
    help: 'Total number of API errors',
    labelNames: ['error_type', 'endpoint'],
  });

  // Custom Metric: Apdex Score (Application Performance Index)
  static apdex = new Gauge({
    name: 'apdex_score',
    help: 'Apdex score (0-1, higher is better)',
    labelNames: ['service'],
  });

  /**
   * Calculate Apdex Score
   * Apdex = (Satisfied + (Tolerating / 2)) / Total
   * Satisfied: Response time <= T
   * Tolerating: Response time > T and <= 4T
   * Frustrated: Response time > 4T
   */
  static calculateApdex(responseTimes: number[], threshold: number = 0.25): number {
    let satisfied = 0;
    let tolerating = 0;
    let frustrated = 0;

    for (const time of responseTimes) {
      if (time <= threshold) {
        satisfied++;
      } else if (time <= threshold * 4) {
        tolerating++;
      } else {
        frustrated++;
      }
    }

    const total = responseTimes.length;
    return (satisfied + tolerating / 2) / total;
  }

  /**
   * Middleware: Instrument HTTP requests
   */
  static instrumentHTTP(req: Request, res: Response, next: NextFunction) {
    const start = Date.now();

    res.on('finish', () => {
      const duration = (Date.now() - start) / 1000;

      this.httpRequestsTotal.inc({
        method: req.method,
        route: req.route?.path || req.path,
        status_code: res.statusCode,
        service: 'vorklee-notes',
      });

      this.httpRequestDuration.observe(
        {
          method: req.method,
          route: req.route?.path || req.path,
          status_code: res.statusCode,
        },
        duration
      );
    });

    next();
  }

  /**
   * Expose metrics endpoint
   */
  static async getMetrics(): Promise<string> {
    return await register.metrics();
  }
}
```

### Grafana Dashboards

```json
// Grafana Dashboard JSON (Service Overview)
{
  "dashboard": {
    "title": "Vorklee2 - Service Overview",
    "tags": ["vorklee", "production"],
    "timezone": "utc",
    "panels": [
      {
        "title": "Request Rate (req/sec)",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{route}}"
          }
        ]
      },
      {
        "title": "Request Duration (P50, P95, P99)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P99"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status_code=~\"5..\"}[5m])",
            "legendFormat": "5xx Errors"
          }
        ]
      },
      {
        "title": "Database Query Duration (P95)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(db_query_duration_seconds_bucket[5m]))",
            "legendFormat": "{{table}}"
          }
        ]
      },
      {
        "title": "Cache Hit Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))",
            "legendFormat": "Hit Rate"
          }
        ]
      },
      {
        "title": "Apdex Score",
        "type": "gauge",
        "targets": [
          {
            "expr": "apdex_score",
            "legendFormat": "{{service}}"
          }
        ],
        "thresholds": [
          { "value": 0.7, "color": "red" },
          { "value": 0.85, "color": "yellow" },
          { "value": 0.94, "color": "green" }
        ]
      }
    ]
  }
}
```

---

## üîç 4. Distributed Tracing (Pillar 3)

### OpenTelemetry Tracing

```typescript
// Distributed Tracing with OpenTelemetry
import { trace, SpanStatusCode } from '@opentelemetry/api';

export class TracingService {
  private tracer = trace.getTracer('vorklee-notes', '5.4.0');

  /**
   * Trace a function execution
   */
  async traceFunction<T>(
    name: string,
    fn: () => Promise<T>,
    attributes: Record<string, any> = {}
  ): Promise<T> {
    const span = this.tracer.startSpan(name, {
      attributes: {
        'service.name': 'vorklee-notes',
        ...attributes,
      },
    });

    try {
      const result = await fn();
      span.setStatus({ code: SpanStatusCode.OK });
      return result;
    } catch (error) {
      span.setStatus({
        code: SpanStatusCode.ERROR,
        message: error.message,
      });
      span.recordException(error);
      throw error;
    } finally {
      span.end();
    }
  }

  /**
   * Trace HTTP request (end-to-end)
   */
  async traceRequest(req: Request, res: Response, next: NextFunction) {
    const span = this.tracer.startSpan(`HTTP ${req.method} ${req.path}`, {
      kind: trace.SpanKind.SERVER,
      attributes: {
        'http.method': req.method,
        'http.url': req.url,
        'http.target': req.path,
        'http.host': req.hostname,
        'http.scheme': req.protocol,
        'http.user_agent': req.headers['user-agent'],
        'http.client_ip': req.ip,
        'user.id': req.user?.id,
        'org.id': req.user?.orgId,
      },
    });

    // Store span in request for child spans
    req.span = span;

    res.on('finish', () => {
      span.setAttributes({
        'http.status_code': res.statusCode,
        'http.response_content_length': res.get('Content-Length'),
      });

      if (res.statusCode >= 500) {
        span.setStatus({ code: SpanStatusCode.ERROR });
      } else {
        span.setStatus({ code: SpanStatusCode.OK });
      }

      span.end();
    });

    next();
  }

  /**
   * Trace database query
   */
  async traceQuery<T>(
    query: string,
    params: any[],
    fn: () => Promise<T>
  ): Promise<T> {
    const span = this.tracer.startSpan('db.query', {
      kind: trace.SpanKind.CLIENT,
      attributes: {
        'db.system': 'postgresql',
        'db.statement': query,
        'db.operation': this.extractOperation(query),
      },
    });

    try {
      const result = await fn();
      span.setStatus({ code: SpanStatusCode.OK });
      return result;
    } catch (error) {
      span.setStatus({
        code: SpanStatusCode.ERROR,
        message: error.message,
      });
      span.recordException(error);
      throw error;
    } finally {
      span.end();
    }
  }

  /**
   * Trace external API call
   */
  async traceExternalAPI<T>(
    service: string,
    method: string,
    url: string,
    fn: () => Promise<T>
  ): Promise<T> {
    const span = this.tracer.startSpan(`External API: ${service}`, {
      kind: trace.SpanKind.CLIENT,
      attributes: {
        'http.method': method,
        'http.url': url,
        'peer.service': service,
      },
    });

    try {
      const result = await fn();
      span.setStatus({ code: SpanStatusCode.OK });
      return result;
    } catch (error) {
      span.setStatus({
        code: SpanStatusCode.ERROR,
        message: error.message,
      });
      span.recordException(error);
      throw error;
    } finally {
      span.end();
    }
  }
}
```

### Trace Sampling Strategy

```typescript
// Intelligent Trace Sampling
export class TraceSampler {
  /**
   * Head-Based Sampling (decide at request start)
   */
  shouldSampleHead(request: Request): boolean {
    // Always sample errors
    if (request.headers['x-force-trace'] === 'true') {
      return true;
    }

    // Sample 100% of POST/PUT/DELETE (write operations)
    if (['POST', 'PUT', 'DELETE'].includes(request.method)) {
      return true;
    }

    // Sample 10% of GET requests
    if (request.method === 'GET') {
      return Math.random() < 0.1;
    }

    return false;
  }

  /**
   * Tail-Based Sampling (decide after request completes)
   * More accurate but requires buffering all spans
   */
  async shouldSampleTail(trace: Trace): Promise<boolean> {
    // Always keep traces with errors
    if (trace.hasErrors) {
      return true;
    }

    // Always keep slow traces (>1s)
    if (trace.duration > 1000) {
      return true;
    }

    // Keep 1% of normal traces
    return Math.random() < 0.01;
  }
}
```

---

## üéØ 5. Service Level Objectives (SLOs)

### SLI/SLO/SLA Definitions

```typescript
// Service Level Objectives
export const SLOs = {
  // API Availability
  availability: {
    sli: 'ratio of successful requests to total requests',
    slo: '99.9%', // 43.2 minutes downtime per month
    sla: '99.5%', // Customer SLA (buffer for error budget)
    measurement: 'sum(rate(http_requests_total{status_code!~"5.."}[30d])) / sum(rate(http_requests_total[30d]))',
  },

  // API Latency
  latency_p95: {
    sli: '95th percentile of request duration',
    slo: '<250ms',
    sla: '<500ms',
    measurement: 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))',
  },

  latency_p99: {
    sli: '99th percentile of request duration',
    slo: '<500ms',
    sla: '<1000ms',
    measurement: 'histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))',
  },

  // Database Performance
  db_latency_p95: {
    sli: '95th percentile of database query duration',
    slo: '<100ms',
    measurement: 'histogram_quantile(0.95, rate(db_query_duration_seconds_bucket[5m]))',
  },

  // Error Budget
  error_budget: {
    calculation: '1 - SLO',
    monthly_allowance: '43.2 minutes (for 99.9% SLO)',
    policy: 'Freeze deployments when error budget exhausted',
  },
};

// SLO Monitoring
export class SLOMonitor {
  /**
   * Calculate Error Budget Remaining
   */
  async getErrorBudget(period: '7d' | '30d' = '30d'): Promise<ErrorBudget> {
    const slo = 0.999; // 99.9%
    const periodMinutes = period === '7d' ? 7 * 24 * 60 : 30 * 24 * 60;

    // Calculate actual uptime
    const uptime = await this.calculateUptime(period);

    // Calculate error budget consumed
    const errorBudgetTotal = (1 - slo) * periodMinutes; // 43.2 minutes for 30d
    const errorBudgetUsed = (1 - uptime) * periodMinutes;
    const errorBudgetRemaining = errorBudgetTotal - errorBudgetUsed;

    return {
      total: errorBudgetTotal,
      used: errorBudgetUsed,
      remaining: errorBudgetRemaining,
      percentage: (errorBudgetRemaining / errorBudgetTotal) * 100,
      status: this.getErrorBudgetStatus(errorBudgetRemaining, errorBudgetTotal),
    };
  }

  /**
   * Error Budget Policy Enforcement
   */
  private getErrorBudgetStatus(remaining: number, total: number): ErrorBudgetStatus {
    const percentage = (remaining / total) * 100;

    if (percentage < 10) {
      return {
        status: 'critical',
        action: 'Freeze all deployments, focus on reliability',
        color: 'red',
      };
    } else if (percentage < 25) {
      return {
        status: 'warning',
        action: 'Reduce deployment velocity, increase testing',
        color: 'yellow',
      };
    } else {
      return {
        status: 'healthy',
        action: 'Normal operations, ship features',
        color: 'green',
      };
    }
  }
}
```

---

## üö® 6. Alerting and On-Call

### Alert Rules (Prometheus Alertmanager)

```yaml
# /etc/prometheus/alerts.yml

groups:
  - name: vorklee_critical
    interval: 30s
    rules:
      # High Error Rate
      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status_code=~"5.."}[5m]) > 0.05
        for: 2m
        labels:
          severity: critical
          service: vorklee-notes
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
          runbook: "https://docs.vorklee.internal/runbooks/high-error-rate"

      # API Latency P95 > 250ms
      - alert: HighAPILatency
        expr: |
          histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.25
        for: 5m
        labels:
          severity: warning
          service: vorklee-notes
        annotations:
          summary: "API latency P95 above SLO"
          description: "P95 latency is {{ $value }}s (SLO: 250ms)"

      # Database Connection Pool Exhausted
      - alert: DatabasePoolExhausted
        expr: |
          db_connection_pool_size{state="waiting"} > 5
        for: 2m
        labels:
          severity: critical
          service: vorklee-notes
        annotations:
          summary: "Database connection pool exhausted"
          description: "{{ $value }} connections waiting for pool"

      # Service Down
      - alert: ServiceDown
        expr: |
          up{job="vorklee-notes"} == 0
        for: 1m
        labels:
          severity: critical
          service: vorklee-notes
        annotations:
          summary: "Service is down"
          description: "Vorklee Notes service has been down for 1 minute"
          runbook: "https://docs.vorklee.internal/runbooks/service-down"

      # Error Budget Exhausted
      - alert: ErrorBudgetExhausted
        expr: |
          (1 - (sum(rate(http_requests_total{status_code!~"5.."}[30d])) / sum(rate(http_requests_total[30d])))) > 0.001
        for: 1h
        labels:
          severity: critical
          service: vorklee-notes
        annotations:
          summary: "Error budget exhausted"
          description: "Monthly error budget has been exhausted. Freeze deployments."
          action: "FREEZE_DEPLOYMENTS"

  - name: vorklee_warning
    interval: 1m
    rules:
      # Cache Hit Rate Low
      - alert: LowCacheHitRate
        expr: |
          rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m])) < 0.8
        for: 10m
        labels:
          severity: warning
          service: vorklee-notes
        annotations:
          summary: "Low cache hit rate"
          description: "Cache hit rate is {{ $value | humanizePercentage }} (threshold: 80%)"

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: |
          (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }}"
```

### On-Call Rotation

```typescript
// PagerDuty Integration
export class OnCallManager {
  private pagerduty: PagerDutyClient;

  /**
   * Trigger incident (P0)
   */
  async triggerIncident(alert: Alert): Promise<Incident> {
    const incident = await this.pagerduty.incidents.create({
      title: alert.title,
      service: { id: 'vorklee-notes', type: 'service_reference' },
      urgency: this.getUrgency(alert.severity),
      body: {
        type: 'incident_body',
        details: alert.description,
      },
      incident_key: alert.id, // For deduplication
    });

    // Notify on-call engineer
    await this.notifyOnCall(incident);

    return incident;
  }

  /**
   * Escalation Policy
   */
  private escalationPolicy = {
    // P0: Immediate escalation
    critical: [
      { level: 1, delay: '0m', notify: ['on-call-primary'] },
      { level: 2, delay: '5m', notify: ['on-call-secondary'] },
      { level: 3, delay: '15m', notify: ['engineering-manager'] },
      { level: 4, delay: '30m', notify: ['cto'] },
    ],

    // P1: Escalate after 30 minutes
    high: [
      { level: 1, delay: '0m', notify: ['on-call-primary'] },
      { level: 2, delay: '30m', notify: ['on-call-secondary'] },
    ],

    // P2: Business hours only
    medium: [
      { level: 1, delay: '0m', notify: ['on-call-primary'], onlyDuringBusinessHours: true },
    ],
  };
}
```

---

## ‚úÖ 7. Summary

This **Advanced Observability and Monitoring** specification provides **complete production visibility** for Vorklee2:

**Key Achievements:**
- **Three Pillars**: Logs (structured, correlated), Metrics (Prometheus), Traces (OpenTelemetry)
- **SLO-Based Monitoring**: 99.9% availability, <250ms P95 latency, error budgets
- **Automated Alerting**: 10+ alert rules with escalation policies
- **Distributed Tracing**: End-to-end request tracing with sampling
- **Business Metrics**: Notes created, active users, Apdex scores
- **On-Call Management**: PagerDuty integration with escalation policies

**Observability Maturity: 100/100** üèÜ

---

**End of File ‚Äî 14_Advanced_Observability_and_Monitoring_v5.md**
